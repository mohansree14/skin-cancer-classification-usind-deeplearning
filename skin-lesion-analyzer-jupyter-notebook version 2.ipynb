{"cells":[{"metadata":{"_uuid":"2852ee80cbebe70f1acb26561531ca5b91f490e4","id":"_Qj9ydmW5zmE"},"cell_type":"markdown","source":["**Changes made in version 2**\n","\n","1. The dataset contains augmented duplicate images. In this version I took steps to ensure that the validation set was made up of unique images only i.e. all the augmented images were in the train set. \n","2. Changed from native Keras to  tensorflow.keras.\n","3. I did not convert the model from Keras to Tensorflowjs in this kernel. This is because I discovered that when a tf.keras model is converted, it does not work in the app. It has to be a native Keras model. Also, the model conversion code ran in the previous kernel but it now produces an error. I think a recent sotware update may be causing this.\n","\n","***\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ba14MewWqu-p","executionInfo":{"status":"ok","timestamp":1685551012149,"user_tz":-330,"elapsed":23919,"user":{"displayName":"mini project","userId":"14257696642339307701"}},"outputId":"7ae06b49-7a6f-43b6-f2b2-5b7ea6b03275"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"metadata":{"_uuid":"d6f77793e7ebbcd523899bafaa9dae9763b9a0a5","id":"t6OodSxA5zmN"},"cell_type":"markdown","source":["**Introduction**\n","\n","I've always wanted to build an end to end ml solution - starting with model creation and ending with a live web app. Here I've managed to do it. Users are able to submit a picture of a skin lesion and get an instant prediction. This kernel details the process I followed to build the model and then convert it from Keras to Tensorflow.js. The javascript, html and css code for the app is available on github. <br>\n","\n","Web App:<br>http://skin.test.woza.work/<br>\n","Github: <br>https://github.com/vbookshelf/Skin-Lesion-Analyzer<br>\n","\n","This model classifies skin lesions into seven classes. It is a fine tuned MobileNet CNN. All training was done in this kernel. The main challenges were the unbalanced dataset and the small amount of data.  I used data augmentation to reduce the class imbalance and in so doing get categorical accuracy scores that were not heavily skewed by a single majority class.\n","\n","MobileNet’s small size and speed makes it ideal for web deployment. It’s also a joy to train.\n","\n","Tensorflow.js is a new library that allows machine learning models to run in the browser - without having to download or install any additional software. Because the model is running locally, any data that a user submits never leaves his or her pc or mobile phone. I imagine that privacy is especially important when it comes to medical data.\n","\n","<hr>\n","\n","**What is the objective?**\n","\n","I found it very helpful to define a clear objective right at the start. This helps guide the model selection process. For example, if a model has an accuracy of 60% it would usually be seen as a bad model. However, if it also has a top 3 accuracy of 90% and the objective requires that it output 3 predictions then it may actually be quite a good model. \n","\n","*This is the objective that I defined for this task:*\n","\n","> Create an online tool that can tell doctors and lab technologists the three highest probability diagnoses for a given skin lesion. This will help them quickly identify high priority patients and speed up their workflow. The app should produce a result in less than 3 seconds. To ensure privacy the images must be pre-processed and analysed locally and never be uploaded to an external server.\n"]},{"cell_type":"code","source":["!pip install tensorflow\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mERA8-ze6AUt","executionInfo":{"status":"ok","timestamp":1685551047230,"user_tz":-330,"elapsed":12457,"user":{"displayName":"mini project","userId":"14257696642339307701"}},"outputId":"5fdee863-32d8-449a-9974-d64f436b16fe"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n","Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"]}]},{"metadata":{"trusted":true,"_uuid":"371731306c3e504b191979706e826c247def88dc","id":"YJYU6Yva5zmP","executionInfo":{"status":"ok","timestamp":1685551129190,"user_tz":-330,"elapsed":5502,"user":{"displayName":"mini project","userId":"14257696642339307701"}}},"cell_type":"code","source":["from numpy.random import seed\n","seed(101)\n","import tensorflow\n","\n","\n","import pandas as pd\n","import numpy as np\n","#import keras\n","#from keras import backend as K\n","\n","import tensorflow\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.metrics import categorical_crossentropy\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","\n","import os\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","import itertools\n","import shutil\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n"],"execution_count":3,"outputs":[]},{"metadata":{"_uuid":"d3f6843b78793e1c047ca6909a7449dc9bfc3f1c","id":"lrnVjl_75zmT"},"cell_type":"markdown","source":["**LABELS**<br>\n","\n","Excerpts from the paper:<br>\n","> The HAM10000 Dataset: A Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions<br>\n","https://arxiv.org/abs/1803.10417\n","\n","\n","\n"," **nv**<br>\n"," Melanocytic nevi are benign neoplasms of melanocytes and appear in a myriad of variants, which all are included in our series. The variants may differ significantly from a dermatoscopic point of view.<br>\n"," *[6705 images]*\n"," \n"," **mel**<br>\n"," Melanoma is a malignant neoplasm derived from melanocytes that may appear in different variants. If excised in an early stage it can be cured by simple surgical excision. Melanomas can be invasive or non-invasive (in situ). We included all variants of melanoma including melanoma in situ, but did exclude non-pigmented, subungual, ocular or mucosal melanoma.<br>*[1113 images]*\n"," \n"," \n","**bkl**<br>\n"," \"Benign keratosis\" is a generic class that includes seborrheic ker- atoses (\"senile wart\"), solar lentigo - which can be regarded a flat variant of seborrheic keratosis - and lichen-planus like keratoses (LPLK), which corresponds to a seborrheic keratosis or a solar lentigo with inflammation\n","and regression [22]. The three subgroups may look different dermatoscop- ically, but we grouped them together because they are similar biologically and often reported under the same generic term histopathologically. From a dermatoscopic view, lichen planus-like keratoses are especially challeng- ing because they can show morphologic features mimicking melanoma [23] and are often biopsied or excised for diagnostic reasons.<br>\n","*[1099 images]*\n","\n","**bcc**<br>\n","Basal cell carcinoma is a common variant of epithelial skin cancer that rarely metastasizes but grows destructively if untreated. It appears in different morphologic variants (flat, nodular, pigmented, cystic, etc) [21], which are all included in this set.<br>\n","*[514 images]*\n"," \n","**akiec**<br>\n","Actinic Keratoses (Solar Keratoses) and intraepithelial Carcinoma (Bowen’s disease) are common non-invasive, variants of squamous cell car- cinoma that can be treated locally without surgery. Some authors regard them as precursors of squamous cell carcinomas and not as actual carci- nomas. There is, however, agreement that these lesions may progress to invasive squamous cell carcinoma - which is usually not pigmented. Both neoplasms commonly show surface scaling and commonly are devoid of pigment. Actinic keratoses are more common on the face and Bowen’s disease is more common on other body sites. Because both types are in- duced by UV-light the surrounding skin is usually typified by severe sun damaged except in cases of Bowen’s disease that are caused by human papilloma virus infection and not by UV. Pigmented variants exists for Bowen’s disease [19] and for actinic keratoses [20]. Both are included in this set.<br>*[327 images]*\n","\n","\n","**vasc**<br>\n","Vascular skin lesions in the dataset range from cherry angiomas to angiokeratomas [25] and pyogenic granulomas [26]. Hemorrhage is also included in this category.<br>\n","*[142 images]*\n","\n","**df**<br>\n","Dermatofibroma is a benign skin lesion regarded as either a benign proliferation or an inflammatory reaction to minimal trauma. It is brown often showing a central zone of fibrosis dermatoscopically [24].<br>*[115 images]*\n","\n","\n","<br>*[Total images = 10015]*"]},{"metadata":{"trusted":true,"_uuid":"d5a0a200bfc57c5489eaa930255d9420a7d01c47","colab":{"base_uri":"https://localhost:8080/"},"id":"GnE0XVJe5zmU","executionInfo":{"status":"ok","timestamp":1685551152167,"user_tz":-330,"elapsed":3,"user":{"displayName":"mini project","userId":"14257696642339307701"}},"outputId":"faad978f-6694-4bc6-929d-b1e880c2e94d"},"cell_type":"code","source":["os.listdir('/content/drive/MyDrive/MINI PROJECT/dataset')"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['HAM10000_metadata.csv',\n"," 'hmnist_28_28_L.csv',\n"," 'hmnist_8_8_L.csv',\n"," 'hmnist_28_28_RGB.csv',\n"," 'hmnist_8_8_RGB.csv',\n"," 'HAM10000_images_part_1',\n"," 'HAM10000_images_part_2']"]},"metadata":{},"execution_count":5}]},{"metadata":{"_uuid":"086162161ba405b800863e7d545b5917e5205984","id":"95GfkqtH5zmV"},"cell_type":"markdown","source":["### Create the directory structure\n","\n","In these folders we will store the images that will later be fed to the Keras generators. "]},{"metadata":{"trusted":true,"_uuid":"d24ef21f9f2359b8bf6b3e7a0b8ab5a43daaf566","id":"SAQGVxcp5zmV","executionInfo":{"status":"ok","timestamp":1685551157535,"user_tz":-330,"elapsed":684,"user":{"displayName":"mini project","userId":"14257696642339307701"}}},"cell_type":"code","source":["\n","\n","# Create a new directory\n","base_dir = 'base_dir'\n","os.mkdir(base_dir)\n","\n","\n","#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n","\n","# now we create 7 folders inside 'base_dir':\n","\n","# train_dir\n","    # nv\n","    # mel\n","    # bkl\n","    # bcc\n","    # akiec\n","    # vasc\n","    # df\n"," \n","# val_dir\n","    # nv\n","    # mel\n","    # bkl\n","    # bcc\n","    # akiec\n","    # vasc\n","    # df\n","\n","# create a path to 'base_dir' to which we will join the names of the new folders\n","# train_dir\n","train_dir = os.path.join(base_dir, 'train_dir')\n","os.mkdir(train_dir)\n","\n","# val_dir\n","val_dir = os.path.join(base_dir, 'val_dir')\n","os.mkdir(val_dir)\n","\n","\n","# [CREATE FOLDERS INSIDE THE TRAIN, VALIDATION AND TEST FOLDERS]\n","# Inside each folder we create seperate folders for each class\n","\n","# create new folders inside train_dir\n","nv = os.path.join(train_dir, 'nv')\n","os.mkdir(nv)\n","mel = os.path.join(train_dir, 'mel')\n","os.mkdir(mel)\n","bkl = os.path.join(train_dir, 'bkl')\n","os.mkdir(bkl)\n","bcc = os.path.join(train_dir, 'bcc')\n","os.mkdir(bcc)\n","akiec = os.path.join(train_dir, 'akiec')\n","os.mkdir(akiec)\n","vasc = os.path.join(train_dir, 'vasc')\n","os.mkdir(vasc)\n","df = os.path.join(train_dir, 'df')\n","os.mkdir(df)\n","\n","\n","\n","# create new folders inside val_dir\n","nv = os.path.join(val_dir, 'nv')\n","os.mkdir(nv)\n","mel = os.path.join(val_dir, 'mel')\n","os.mkdir(mel)\n","bkl = os.path.join(val_dir, 'bkl')\n","os.mkdir(bkl)\n","bcc = os.path.join(val_dir, 'bcc')\n","os.mkdir(bcc)\n","akiec = os.path.join(val_dir, 'akiec')\n","os.mkdir(akiec)\n","vasc = os.path.join(val_dir, 'vasc')\n","os.mkdir(vasc)\n","df = os.path.join(val_dir, 'df')\n","os.mkdir(df)\n","\n"],"execution_count":6,"outputs":[]},{"metadata":{"_uuid":"4ae8d37fdee293aaffa71a79019dd7277f8288fc","id":"N6UGKKkt5zmW"},"cell_type":"markdown","source":["### Create Train and Val Sets"]},{"metadata":{"trusted":true,"_uuid":"268503398ef61904e05a2c0b0667d589f08a19a8","id":"z9CWYYVO5zmX","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1685551188158,"user_tz":-330,"elapsed":723,"user":{"displayName":"mini project","userId":"14257696642339307701"}},"outputId":"bcb7a3b3-c8d6-4719-a053-18be555a8bc9"},"cell_type":"code","source":["df_data = pd.read_csv('/content/drive/MyDrive/MINI PROJECT/dataset/HAM10000_metadata.csv')\n","\n","df_data.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     lesion_id      image_id   dx dx_type   age   sex localization\n","0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n","1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n","2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n","3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n","4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear"],"text/html":["\n","  <div id=\"df-742ad1d0-069f-452c-90c3-99ad38f0286a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>lesion_id</th>\n","      <th>image_id</th>\n","      <th>dx</th>\n","      <th>dx_type</th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>localization</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>HAM_0000118</td>\n","      <td>ISIC_0027419</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>HAM_0000118</td>\n","      <td>ISIC_0025030</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>HAM_0002730</td>\n","      <td>ISIC_0026769</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>HAM_0002730</td>\n","      <td>ISIC_0025661</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>HAM_0001466</td>\n","      <td>ISIC_0031633</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>75.0</td>\n","      <td>male</td>\n","      <td>ear</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-742ad1d0-069f-452c-90c3-99ad38f0286a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-742ad1d0-069f-452c-90c3-99ad38f0286a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-742ad1d0-069f-452c-90c3-99ad38f0286a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"metadata":{"_uuid":"c46ea5967e095d31dcf144b6f57f0343878fa432","id":"qBYDyrEy5zmY"},"cell_type":"markdown","source":["### Create a stratified val set"]},{"metadata":{"trusted":true,"_uuid":"53e4b7b152ed831a7d7516156ac300c0e6985ffc","id":"7bDaDv7G5zmZ","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1685551192150,"user_tz":-330,"elapsed":6,"user":{"displayName":"mini project","userId":"14257696642339307701"}},"outputId":"3ecde5be-3285-4351-e82d-8183dd2e48ff"},"cell_type":"code","source":["# this will tell us how many images are associated with each lesion_id\n","df = df_data.groupby('lesion_id').count()\n","\n","# now we filter out lesion_id's that have only one image associated with it\n","df = df[df['image_id'] == 1]\n","\n","df.reset_index(inplace=True)\n","\n","df.head()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     lesion_id  image_id  dx  dx_type  age  sex  localization\n","0  HAM_0000001         1   1        1    1    1             1\n","1  HAM_0000003         1   1        1    1    1             1\n","2  HAM_0000004         1   1        1    1    1             1\n","3  HAM_0000007         1   1        1    1    1             1\n","4  HAM_0000008         1   1        1    1    1             1"],"text/html":["\n","  <div id=\"df-61ec4544-3b67-401b-9285-edfb84fb1f49\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>lesion_id</th>\n","      <th>image_id</th>\n","      <th>dx</th>\n","      <th>dx_type</th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>localization</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>HAM_0000001</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>HAM_0000003</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>HAM_0000004</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>HAM_0000007</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>HAM_0000008</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61ec4544-3b67-401b-9285-edfb84fb1f49')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-61ec4544-3b67-401b-9285-edfb84fb1f49 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-61ec4544-3b67-401b-9285-edfb84fb1f49');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}]},{"metadata":{"trusted":true,"_uuid":"24720fe3ea9f2f4b571abd09ecfbb931d6429852","id":"T49LMLTd5zmZ","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1685551201725,"user_tz":-330,"elapsed":5084,"user":{"displayName":"mini project","userId":"14257696642339307701"}},"outputId":"8417509c-bdfe-4703-a1bd-ec522d3d4276"},"cell_type":"code","source":["# here we identify lesion_id's that have duplicate images and those that have only\n","# one image.\n","\n","def identify_duplicates(x):\n","    \n","    unique_list = list(df['lesion_id'])\n","    \n","    if x in unique_list:\n","        return 'no_duplicates'\n","    else:\n","        return 'has_duplicates'\n","    \n","# create a new colum that is a copy of the lesion_id column\n","df_data['duplicates'] = df_data['lesion_id']\n","# apply the function to this new column\n","df_data['duplicates'] = df_data['duplicates'].apply(identify_duplicates)\n","\n","df_data.head()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     lesion_id      image_id   dx dx_type   age   sex localization  \\\n","0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n","1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n","2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n","3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n","4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n","\n","       duplicates  \n","0  has_duplicates  \n","1  has_duplicates  \n","2  has_duplicates  \n","3  has_duplicates  \n","4  has_duplicates  "],"text/html":["\n","  <div id=\"df-946c8a76-bd62-47d6-b002-59f3d3a83434\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>lesion_id</th>\n","      <th>image_id</th>\n","      <th>dx</th>\n","      <th>dx_type</th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>localization</th>\n","      <th>duplicates</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>HAM_0000118</td>\n","      <td>ISIC_0027419</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","      <td>has_duplicates</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>HAM_0000118</td>\n","      <td>ISIC_0025030</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","      <td>has_duplicates</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>HAM_0002730</td>\n","      <td>ISIC_0026769</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","      <td>has_duplicates</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>HAM_0002730</td>\n","      <td>ISIC_0025661</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","      <td>has_duplicates</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>HAM_0001466</td>\n","      <td>ISIC_0031633</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>75.0</td>\n","      <td>male</td>\n","      <td>ear</td>\n","      <td>has_duplicates</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-946c8a76-bd62-47d6-b002-59f3d3a83434')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-946c8a76-bd62-47d6-b002-59f3d3a83434 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-946c8a76-bd62-47d6-b002-59f3d3a83434');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"metadata":{"trusted":true,"_uuid":"08b7eef3e0ac4112f63b8fb26ce19d55483cbc04","id":"Z31aWeD-5zma","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685551206675,"user_tz":-330,"elapsed":400,"user":{"displayName":"mini project","userId":"14257696642339307701"}},"outputId":"29ac61b7-a74f-497f-bef5-4459cfc4e64d"},"cell_type":"code","source":["df_data['duplicates'].value_counts()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["no_duplicates     5514\n","has_duplicates    4501\n","Name: duplicates, dtype: int64"]},"metadata":{},"execution_count":10}]},{"metadata":{"trusted":true,"_uuid":"995445dfda2745165a53e61f42615104b951d4af","id":"hQkOi5J05zma","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685551210570,"user_tz":-330,"elapsed":638,"user":{"displayName":"mini project","userId":"14257696642339307701"}},"outputId":"f1e144a7-b363-4712-d566-a1641c371fc4"},"cell_type":"code","source":["# now we filter out images that don't have duplicates\n","df = df_data[df_data['duplicates'] == 'no_duplicates']\n","\n","df.shape"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5514, 8)"]},"metadata":{},"execution_count":11}]},{"metadata":{"trusted":true,"_uuid":"39fde25b59a9452cf700c5b2ff82cc7cc45c4a33","id":"dd1zYCwt5zmb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685551214698,"user_tz":-330,"elapsed":672,"user":{"displayName":"mini project","userId":"14257696642339307701"}},"outputId":"e1b0f670-eb85-4df6-a9bd-83c2c6a48fb9"},"cell_type":"code","source":["# now we create a val set using df because we are sure that none of these images\n","# have augmented duplicates in the train set\n","y = df['dx']\n","\n","_, df_val = train_test_split(df, test_size=0.17, random_state=101, stratify=y)\n","\n","df_val.shape"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(938, 8)"]},"metadata":{},"execution_count":12}]},{"metadata":{"trusted":true,"_uuid":"1df37227f7ce993d054ed5b8480ee724696fc210","id":"BuSmP5Ex5zmb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685551218852,"user_tz":-330,"elapsed":3,"user":{"displayName":"mini project","userId":"14257696642339307701"}},"outputId":"1c435f05-0e8c-4384-cc0f-13679663da2b"},"cell_type":"code","source":["df_val['dx'].value_counts()"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["nv       751\n","bkl       75\n","mel       39\n","bcc       30\n","akiec     26\n","vasc      11\n","df         6\n","Name: dx, dtype: int64"]},"metadata":{},"execution_count":13}]},{"metadata":{"_uuid":"08c5e12fcef2da5f49267a6b82161b2c52c2b20a","id":"H-7Fw60j5zmb"},"cell_type":"markdown","source":["### Create a train set that excludes images that are in the val set"]},{"metadata":{"trusted":true,"_uuid":"03715a6cf5aeb6430ee144a84eb10dde216c0fb9","id":"RkXvJQNB5zmc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685551227556,"user_tz":-330,"elapsed":1187,"user":{"displayName":"mini project","userId":"14257696642339307701"}},"outputId":"d3067921-5453-424f-b44f-4cc41ce94557"},"cell_type":"code","source":["# This set will be df_data excluding all rows that are in the val set\n","\n","# This function identifies if an image is part of the train\n","# or val set.\n","def identify_val_rows(x):\n","    # create a list of all the lesion_id's in the val set\n","    val_list = list(df_val['image_id'])\n","    \n","    if str(x) in val_list:\n","        return 'val'\n","    else:\n","        return 'train'\n","\n","# identify train and val rows\n","\n","# create a new colum that is a copy of the image_id column\n","df_data['train_or_val'] = df_data['image_id']\n","# apply the function to this new column\n","df_data['train_or_val'] = df_data['train_or_val'].apply(identify_val_rows)\n","   \n","# filter out train rows\n","df_train = df_data[df_data['train_or_val'] == 'train']\n","\n","\n","print(len(df_train))\n","print(len(df_val))"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["9077\n","938\n"]}]},{"metadata":{"trusted":true,"_uuid":"4b976a9018b1bd2dc0522c68339c5861534a1571","id":"547gCz9s5zmc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685551232697,"user_tz":-330,"elapsed":700,"user":{"displayName":"mini project","userId":"14257696642339307701"}},"outputId":"b689a78a-6c71-42cd-ad9b-c85f74f68059"},"cell_type":"code","source":["df_train['dx'].value_counts()"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["nv       5954\n","mel      1074\n","bkl      1024\n","bcc       484\n","akiec     301\n","vasc      131\n","df        109\n","Name: dx, dtype: int64"]},"metadata":{},"execution_count":15}]},{"metadata":{"trusted":true,"_uuid":"1581d5a3e86f9673ae175102112017e30229bc37","id":"jazATjbf5zmc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685551236046,"user_tz":-330,"elapsed":4,"user":{"displayName":"mini project","userId":"14257696642339307701"}},"outputId":"b03fd8aa-c8e5-4a15-e6dd-fa0c095a8a07"},"cell_type":"code","source":["df_val['dx'].value_counts()"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["nv       751\n","bkl       75\n","mel       39\n","bcc       30\n","akiec     26\n","vasc      11\n","df         6\n","Name: dx, dtype: int64"]},"metadata":{},"execution_count":16}]},{"metadata":{"_uuid":"8812ad87c4fa18d2d82497df42c3895c7f10bc39","id":"XKXqcoGh5zmd"},"cell_type":"markdown","source":["### Transfer the Images into the Folders"]},{"metadata":{"trusted":true,"_uuid":"4acee2b7879762e50b52df118a9b691515fe7ac0","id":"1E-t8N9a5zmd","executionInfo":{"status":"ok","timestamp":1685551240887,"user_tz":-330,"elapsed":1061,"user":{"displayName":"mini project","userId":"14257696642339307701"}}},"cell_type":"code","source":["# Set the image_id as the index in df_data\n","df_data.set_index('image_id', inplace=True)"],"execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eca02fbf066c8124d0cb465295bbd2593f5f045a","id":"h5sW1fq15zmd","colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"status":"error","timestamp":1685553340457,"user_tz":-330,"elapsed":72211,"user":{"displayName":"mini project","userId":"14257696642339307701"}},"outputId":"2fa82d52-2a44-4742-dc19-d2e092fef88c"},"cell_type":"code","source":["# Get a list of images in each of the two folders\n","folder_1 = os.listdir('/content/drive/MyDrive/MINI PROJECT/dataset/HAM10000_images_part_1')\n","folder_2 = os.listdir('/content/drive/MyDrive/MINI PROJECT/dataset/HAM10000_images_part_2')\n","\n","# Get a list of train and val images\n","train_list = list(df_train['image_id'])\n","val_list = list(df_val['image_id'])\n","\n","\n","\n","# Transfer the train images\n","\n","for image in train_list:\n","    \n","    fname = image + '.jpg'\n","    label = df_data.loc[image,'dx']\n","    \n","    if fname in folder_1:\n","        # source path to image\n","        src = os.path.join('/content/drive/MyDrive/MINI PROJECT/dataset/HAM10000_images_part_1', fname)\n","        # destination path to image\n","        dst = os.path.join(train_dir, label, fname)\n","        # copy the image from the source to the destination\n","        shutil.copyfile(src, dst)\n","\n","    if fname in folder_2:\n","        # source path to image\n","        src = os.path.join('/content/drive/MyDrive/MINI PROJECT/dataset/HAM10000_images_part_2', fname)\n","        # destination path to image\n","        dst = os.path.join(train_dir, label, fname)\n","        # copy the image from the source to the destination\n","        shutil.copyfile(src, dst)\n","\n","\n","# Transfer the val images\n","\n","for image in val_list:\n","    \n","    fname = image + '.jpg'\n","    label = df_data.loc[image,'dx']\n","    \n","    if fname in folder_1:\n","        # source path to image\n","        src = os.path.join('/content/drive/MyDrive/MINI PROJECT/dataset/HAM10000_images_part_1', fname)\n","        # destination path to image\n","        dst = os.path.join(val_dir, label, fname)\n","        # copy the image from the source to the destination\n","        shutil.copyfile(src, dst)\n","\n","    if fname in folder_2:\n","        # source path to image\n","        src = os.path.join('/content/drive/MyDrive/MINI PROJECT/dataset/HAM10000_images_part_2', fname)\n","        # destination path to image\n","        dst = os.path.join(val_dir, label, fname)\n","        # copy the image from the source to the destination\n","        shutil.copyfile(src, dst)\n","        "],"execution_count":20,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-694d92dd5634>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# copy the image from the source to the destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m                             \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true,"_uuid":"5a4847c4cc799c23e57bf2531d92117cb95e1b07","id":"ydrI3hce5zmd"},"cell_type":"code","source":["# check how many train images we have in each folder\n","\n","print(len(os.listdir('base_dir/train_dir/nv')))\n","print(len(os.listdir('base_dir/train_dir/mel')))\n","print(len(os.listdir('base_dir/train_dir/bkl')))\n","print(len(os.listdir('base_dir/train_dir/bcc')))\n","print(len(os.listdir('base_dir/train_dir/akiec')))\n","print(len(os.listdir('base_dir/train_dir/vasc')))\n","print(len(os.listdir('base_dir/train_dir/df')))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd05c08cbfa00418dc333f5b67d1ff6e98aa973e","id":"098MHRjG5zmd"},"cell_type":"code","source":["# check how many val images we have in each folder\n","\n","print(len(os.listdir('base_dir/val_dir/nv')))\n","print(len(os.listdir('base_dir/val_dir/mel')))\n","print(len(os.listdir('base_dir/val_dir/bkl')))\n","print(len(os.listdir('base_dir/val_dir/bcc')))\n","print(len(os.listdir('base_dir/val_dir/akiec')))\n","print(len(os.listdir('base_dir/val_dir/vasc')))\n","print(len(os.listdir('base_dir/val_dir/df')))"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cebcb5242ff542efb03be5086bf3796bea70c591","id":"L6EvFK3s5zme"},"cell_type":"markdown","source":["### Copy the train images  into aug_dir"]},{"metadata":{"trusted":true,"_uuid":"8fe970d74e9d5a284420af4ad37d8aae89dc1c15","id":"9DfkoRou5zme"},"cell_type":"code","source":["# note that we are not augmenting class 'nv'\n","class_list = ['mel','bkl','bcc','akiec','vasc','df']\n","\n","for item in class_list:\n","    \n","    # We are creating temporary directories here because we delete these directories later\n","    # create a base dir\n","    aug_dir = 'aug_dir'\n","    os.mkdir(aug_dir)\n","    # create a dir within the base dir to store images of the same class\n","    img_dir = os.path.join(aug_dir, 'img_dir')\n","    os.mkdir(img_dir)\n","\n","    # Choose a class\n","    img_class = item\n","\n","    # list all images in that directory\n","    img_list = os.listdir('base_dir/train_dir/' + img_class)\n","\n","    # Copy images from the class train dir to the img_dir e.g. class 'mel'\n","    for fname in img_list:\n","            # source path to image\n","            src = os.path.join('base_dir/train_dir/' + img_class, fname)\n","            # destination path to image\n","            dst = os.path.join(img_dir, fname)\n","            # copy the image from the source to the destination\n","            shutil.copyfile(src, dst)\n","\n","\n","    # point to a dir containing the images and not to the images themselves\n","    path = aug_dir\n","    save_path = 'base_dir/train_dir/' + img_class\n","\n","    # Create a data generator\n","    datagen = ImageDataGenerator(\n","        rotation_range=180,\n","        width_shift_range=0.1,\n","        height_shift_range=0.1,\n","        zoom_range=0.1,\n","        horizontal_flip=True,\n","        vertical_flip=True,\n","        #brightness_range=(0.9,1.1),\n","        fill_mode='nearest')\n","\n","    batch_size = 50\n","\n","    aug_datagen = datagen.flow_from_directory(path,\n","                                           save_to_dir=save_path,\n","                                           save_format='jpg',\n","                                                    target_size=(224,224),\n","                                                    batch_size=batch_size)\n","\n","\n","\n","    # Generate the augmented images and add them to the training folders\n","    \n","    ###########\n","    \n","    num_aug_images_wanted = 6000 # total number of images we want to have in each class\n","    \n","    ###########\n","    \n","    num_files = len(os.listdir(img_dir))\n","    num_batches = int(np.ceil((num_aug_images_wanted-num_files)/batch_size))\n","\n","    # run the generator and create about 6000 augmented images\n","    for i in range(0,num_batches):\n","\n","        imgs, labels = next(aug_datagen)\n","        \n","    # delete temporary directory with the raw image files\n","    shutil.rmtree('aug_dir')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9bbc56bd25441150d2430dca2b07d8ebae57d95","id":"6yFZKKjv5zme"},"cell_type":"code","source":["# Check how many train images we now have in each folder.\n","# This is the original images plus the augmented images.\n","\n","print(len(os.listdir('base_dir/train_dir/nv')))\n","print(len(os.listdir('base_dir/train_dir/mel')))\n","print(len(os.listdir('base_dir/train_dir/bkl')))\n","print(len(os.listdir('base_dir/train_dir/bcc')))\n","print(len(os.listdir('base_dir/train_dir/akiec')))\n","print(len(os.listdir('base_dir/train_dir/vasc')))\n","print(len(os.listdir('base_dir/train_dir/df')))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21de03bdc63ecf78cc061d364d14d3216a544b43","id":"S-cYd65c5zme"},"cell_type":"code","source":["# Check how many val images we have in each folder.\n","\n","print(len(os.listdir('base_dir/val_dir/nv')))\n","print(len(os.listdir('base_dir/val_dir/mel')))\n","print(len(os.listdir('base_dir/val_dir/bkl')))\n","print(len(os.listdir('base_dir/val_dir/bcc')))\n","print(len(os.listdir('base_dir/val_dir/akiec')))\n","print(len(os.listdir('base_dir/val_dir/vasc')))\n","print(len(os.listdir('base_dir/val_dir/df')))"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"767cb7d35e301369f020cdbb705da1620ba8e594","id":"tEWXq6Rg5zme"},"cell_type":"markdown","source":["### Visualize 50 augmented images"]},{"metadata":{"trusted":true,"_uuid":"5f0e13a8455af926fe449e1b3ea818b704724202","id":"4c8y2OxV5zmf"},"cell_type":"code","source":["# plots images with labels within jupyter notebook\n","# source: https://github.com/smileservices/keras_utils/blob/master/utils.py\n","\n","def plots(ims, figsize=(12,6), rows=5, interp=False, titles=None): # 12,6\n","    if type(ims[0]) is np.ndarray:\n","        ims = np.array(ims).astype(np.uint8)\n","        if (ims.shape[-1] != 3):\n","            ims = ims.transpose((0,2,3,1))\n","    f = plt.figure(figsize=figsize)\n","    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n","    for i in range(len(ims)):\n","        sp = f.add_subplot(rows, cols, i+1)\n","        sp.axis('Off')\n","        if titles is not None:\n","            sp.set_title(titles[i], fontsize=16)\n","        plt.imshow(ims[i], interpolation=None if interp else 'none')\n","        \n","plots(imgs, titles=None) # titles=labels will display the image labels"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3e2126a39c06568a1f95da2ab42353447d1be20","id":"PbctYVKA5zmf"},"cell_type":"code","source":["# End of Data Preparation\n","### ===================================================================================== ###\n","# Start of Model Building"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32dad10b7c104d2baa972da8cbadc7d6038af05c","id":"vxjhjfrz5zmf"},"cell_type":"markdown","source":["### Set Up the Generators"]},{"metadata":{"trusted":true,"_uuid":"aa1041d69b0e8313324b91e3e9475799e1ad61c2","id":"-K-1xNaY5zmf"},"cell_type":"code","source":["train_path = 'base_dir/train_dir'\n","valid_path = 'base_dir/val_dir'\n","\n","num_train_samples = len(df_train)\n","num_val_samples = len(df_val)\n","train_batch_size = 10\n","val_batch_size = 10\n","image_size = 224\n","\n","train_steps = np.ceil(num_train_samples / train_batch_size)\n","val_steps = np.ceil(num_val_samples / val_batch_size)\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0e5aede7139196b0d4e1344b278e7621f005550","id":"hSeXDws_5zmf"},"cell_type":"code","source":["\n","datagen = ImageDataGenerator(\n","    preprocessing_function= \\\n","    tensorflow.keras.applications.mobilenet.preprocess_input)\n","\n","train_batches = datagen.flow_from_directory(train_path,\n","                                            target_size=(image_size,image_size),\n","                                            batch_size=train_batch_size)\n","\n","valid_batches = datagen.flow_from_directory(valid_path,\n","                                            target_size=(image_size,image_size),\n","                                            batch_size=val_batch_size)\n","\n","# Note: shuffle=False causes the test dataset to not be shuffled\n","test_batches = datagen.flow_from_directory(valid_path,\n","                                            target_size=(image_size,image_size),\n","                                            batch_size=1,\n","                                            shuffle=False)"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ee4ee41f1b16083bd9fc20ee9dec40acccc97dd","id":"dXVK5SGt5zmf"},"cell_type":"markdown","source":["### Modify MobileNet Model"]},{"metadata":{"trusted":true,"_uuid":"ad582cb8ea0ca2d563fc367aa89b7edfafc1a57f","id":"0_R-AAxP5zmf"},"cell_type":"code","source":["# create a copy of a mobilenet model\n","\n","mobile = tensorflow.keras.applications.mobilenet.MobileNet()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"960449ec7ecdda92ba733ad23b00b7be605f3d4b","_kg_hide-output":true,"id":"OVFwmSUC5zmg"},"cell_type":"code","source":["mobile.summary()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b7922bdf625675834d9b63ec0e85351bd9f3c0f","id":"YGYbR4z55zmg"},"cell_type":"code","source":["type(mobile.layers)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f832e5865c65a013a06dbf5d500c0381020c56d5","id":"39uWVMzj5zmm"},"cell_type":"code","source":["# How many layers does MobileNet have?\n","len(mobile.layers)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dd9dcf26d85a57a113e6b158cf8fceeca7f99de","id":"1FU4wPEy5zmm"},"cell_type":"code","source":["# CREATE THE MODEL ARCHITECTURE\n","\n","# Exclude the last 5 layers of the above model.\n","# This will include all layers up to and including global_average_pooling2d_1\n","x = mobile.layers[-6].output\n","\n","# Create a new dense layer for predictions\n","# 7 corresponds to the number of classes\n","x = Dropout(0.25)(x)\n","predictions = Dense(7, activation='softmax')(x)\n","\n","# inputs=mobile.input selects the input layer, outputs=predictions refers to the\n","# dense layer we created above.\n","\n","model = Model(inputs=mobile.input, outputs=predictions)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b38734b72afc4289ab187a9e683cbda6bf3269bc","_kg_hide-output":true,"id":"BPN7m1vM5zmm"},"cell_type":"code","source":["model.summary()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9d74e44630c3d07a596460c8fbfda3ae7cae1e9","id":"PQPllr415zmm"},"cell_type":"code","source":["# We need to choose how many layers we actually want to be trained.\n","\n","# Here we are freezing the weights of all layers except the\n","# last 23 layers in the new model.\n","# The last 23 layers of the model will be trained.\n","\n","for layer in model.layers[:-23]:\n","    layer.trainable = False"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13cf63a53e5195cb8a9725d2506c71108bc478b9","id":"XE8a9yeu5zmn"},"cell_type":"markdown","source":["### Train the Model"]},{"metadata":{"trusted":true,"_uuid":"915f30a4f5ad369713bcb8e3bfa438219d6c8ef7","id":"XfUPWKLz5zmn"},"cell_type":"code","source":["# Define Top2 and Top3 Accuracy\n","\n","from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n","\n","def top_3_accuracy(y_true, y_pred):\n","    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n","\n","def top_2_accuracy(y_true, y_pred):\n","    return top_k_categorical_accuracy(y_true, y_pred, k=2)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2013ff1abae70fed845af94e7ab3d95cefad0d61","id":"iN7mlv8U5zmn"},"cell_type":"code","source":["model.compile(Adam(lr=0.01), loss='categorical_crossentropy', \n","              metrics=[categorical_accuracy, top_2_accuracy, top_3_accuracy])\n","\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62e7a784a33d4c868f49a3ef1f9acbc7186e3338","id":"MD3956y25zmn"},"cell_type":"code","source":["# Get the labels that are associated with each index\n","print(valid_batches.class_indices)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3001857c9a3c2b15c2343627e340eb1ae858fae9","id":"8WFkoxHG5zmn"},"cell_type":"code","source":["# Add weights to try to make the model more sensitive to melanoma\n","\n","class_weights={\n","    0: 1.0, # akiec\n","    1: 1.0, # bcc\n","    2: 1.0, # bkl\n","    3: 1.0, # df\n","    4: 3.0, # mel # Try to make the model more sensitive to Melanoma.\n","    5: 1.0, # nv\n","    6: 1.0, # vasc\n","}"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a5e3bc3cf44f1d4326c34ad880a302ba082e9d5","scrolled":false,"_kg_hide-output":true,"id":"lZZGLM_15zmn"},"cell_type":"code","source":["\n","filepath = \"model.h5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='val_top_3_accuracy', verbose=1, \n","                             save_best_only=True, mode='max')\n","\n","reduce_lr = ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.5, patience=2, \n","                                   verbose=1, mode='max', min_lr=0.00001)\n","                              \n","                              \n","callbacks_list = [checkpoint, reduce_lr]\n","\n","history = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n","                              class_weight=class_weights,\n","                    validation_data=valid_batches,\n","                    validation_steps=val_steps,\n","                    epochs=30, verbose=1,\n","                   callbacks=callbacks_list)\n"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3e43e3f2943db4be9d75831fe23661ae9deb44b","id":"uXDBM7Qx5zmo"},"cell_type":"markdown","source":["### Evaluate the model using the val set"]},{"metadata":{"trusted":true,"_uuid":"710ee26097924153647ac432c8ade29383fe42f1","id":"omnjBpO25zmo"},"cell_type":"code","source":["# get the metric names so we can use evaulate_generator\n","model.metrics_names"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68603a5e8cb5e507db95074a07b552a61fa48e11","id":"axCnCSfM5zmo"},"cell_type":"code","source":["# Here the the last epoch will be used.\n","\n","val_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\n","model.evaluate_generator(test_batches, \n","                        steps=len(df_val))\n","\n","print('val_loss:', val_loss)\n","print('val_cat_acc:', val_cat_acc)\n","print('val_top_2_acc:', val_top_2_acc)\n","print('val_top_3_acc:', val_top_3_acc)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"897f066da922d81fefa165a6b911a741c52ef7f5","id":"C-COSP5L5zmo"},"cell_type":"code","source":["# Here the best epoch will be used.\n","\n","model.load_weights('model.h5')\n","\n","val_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\n","model.evaluate_generator(test_batches, \n","                        steps=len(df_val))\n","\n","print('val_loss:', val_loss)\n","print('val_cat_acc:', val_cat_acc)\n","print('val_top_2_acc:', val_top_2_acc)\n","print('val_top_3_acc:', val_top_3_acc)"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3fffba5e0aa9088cda1865c7b8d75d72c20d0f6","id":"PyobFgOl5zmo"},"cell_type":"markdown","source":["### Plot the Training Curves"]},{"metadata":{"trusted":true,"_uuid":"0cbd11ef4286a751ef2918361af035d356f341ae","id":"ky7Vmqip5zmo"},"cell_type":"code","source":["# display the loss and accuracy curves\n","\n","import matplotlib.pyplot as plt\n","\n","acc = history.history['categorical_accuracy']\n","val_acc = history.history['val_categorical_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","train_top2_acc = history.history['top_2_accuracy']\n","val_top2_acc = history.history['val_top_2_accuracy']\n","train_top3_acc = history.history['top_3_accuracy']\n","val_top3_acc = history.history['val_top_3_accuracy']\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","plt.figure()\n","\n","plt.plot(epochs, acc, 'bo', label='Training cat acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation cat acc')\n","plt.title('Training and validation cat accuracy')\n","plt.legend()\n","plt.figure()\n","\n","\n","plt.plot(epochs, train_top2_acc, 'bo', label='Training top2 acc')\n","plt.plot(epochs, val_top2_acc, 'b', label='Validation top2 acc')\n","plt.title('Training and validation top2 accuracy')\n","plt.legend()\n","plt.figure()\n","plt.plot(epochs, train_top3_acc, 'bo', label='Training top3 acc')\n","plt.plot(epochs, val_top3_acc, 'b', label='Validation top3 acc')\n","plt.title('Training and validation top3 accuracy')\n","plt.legend()\n","\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4204e4056c8d12c1fee72b97912879cad4ee483f","id":"9rsCLk4f5zmp"},"cell_type":"markdown","source":["### Create a Confusion Matrix"]},{"metadata":{"trusted":true,"_uuid":"74a66905f7a2d702f3d2aad9abf9fe114b96f0ff","id":"NjOJGE0E5zmp"},"cell_type":"code","source":["# Get the labels of the test images.\n","# Note that cats and dogs are in seperate folders therefore\n","# the code below can get the labels depending on the folder the image is in.\n","\n","test_labels = test_batches.classes"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53f4b22617285e923f336cdb2ffcbe1f9ff5e5db","_kg_hide-output":true,"id":"cjFbGg565zmp"},"cell_type":"code","source":["# We need these to plot the confusion matrix.\n","test_labels"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5113e039e8384b96595751e084f0c5ed677080a","id":"c51oVrTO5zmp"},"cell_type":"code","source":["# Print the label associated with each class\n","test_batches.class_indices"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"701dafc5874aa60a054a74c04170cb7e8d750e94","id":"0Cz59yFd5zmp"},"cell_type":"code","source":["# make a prediction\n","predictions = model.predict_generator(test_batches, steps=len(df_val), verbose=1)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcce17ac0488ff90d29b11592c9226ed1bb210fb","id":"oXOKqYQ-5zmq"},"cell_type":"code","source":["predictions.shape"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cfd9bdbbd27e27d9c5de7c6593527686445ea89","id":"yjdofwjS5zmq"},"cell_type":"code","source":["# Source: Scikit Learn website\n","# http://scikit-learn.org/stable/auto_examples/\n","# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n","# selection-plot-confusion-matrix-py\n","\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.tight_layout()\n","\n","\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d499136cdb5fdf356515beb6e0cd1130ed584db","id":"9BCbR6CU5zmq"},"cell_type":"code","source":["test_labels.shape"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"940b71bb2b37d847ba81dd67ca50c7fd5785fd35","id":"bK0biBGE5zmq"},"cell_type":"code","source":["# argmax returns the index of the max value in a row\n","cm = confusion_matrix(test_labels, predictions.argmax(axis=1))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97c6b493c368ff6565782c1bb15827f5d349ef79","id":"RiQJbl1B5zmq"},"cell_type":"code","source":["test_batches.class_indices"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ddbd33a93468075c64ba49188a6d272a5c7828f","id":"1FLJhA0t5zmq"},"cell_type":"code","source":["# Define the labels of the class indices. These need to match the \n","# order shown above.\n","cm_plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel','nv', 'vasc']\n","\n","plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c46f3f1d257241f96b4aac7eb96831ff8bbea33","id":"gMLu6Q_15zmq"},"cell_type":"code","source":["# End of Model Building\n","### ===================================================================================== ###\n","# Convert the Model from Keras to Tensorflow.js"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f9017d69bf0b84522e34841c1876b613cae1535","id":"O8Z7gDBH5zmr"},"cell_type":"markdown","source":["### Install Tensorflow.js"]},{"metadata":{"trusted":true,"_uuid":"2da93a52657b786a8eb7a0d5df6d6a2bcbd0f1c6","_kg_hide-output":true,"id":"s2EKeS5a5zmr"},"cell_type":"code","source":["#!pip install tensorflowjs"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a85d7889e2bada2ebe3b84fc1571a89b1a66b7b0","id":"NgCITEml5zmr"},"cell_type":"markdown","source":["### Convert the model from Keras to Tensorflowjs\n","The conversion code below no longer works in kaggle kernels. I've left it in for reference. \n","\n","In order to convert this model the workaround is as follows:<br>\n","1. Recreate the model using native Keras.<br>\n","2. Use the command line conversion process to convert the model from Keras to Tensorflowjs.<br>\n","Here's how to do that: https://www.youtube.com/watch?v=Kc2_x6pBYGE\n","\n","The above steps can be done in a kaggle kernel quite easily. Tensorflowjs is still fairly new so these type of bugs are not unusual."]},{"metadata":{"trusted":true,"_uuid":"9977179251a2feb129a946028fb74d30b9eb7341","id":"H_IQ8Yhs5zmr"},"cell_type":"code","source":["# create a directory to store the model files\n","#os.mkdir('tfjs_dir')\n","\n","# convert to Tensorflow.js\n","#import tensorflowjs as tfjs\n","\n","# Error\n","# AttributeError: module 'tensorflow.python.data.ops.dataset_ops' \n","    # has no attribute 'UnaryDataset'\n","\n","#tfjs.converters.save_keras_model(model, 'tfjs_dir')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7df391a2792ddfb7fa2a980776aeac744612f702","id":"QxhXRBke5zmr"},"cell_type":"code","source":["# check the the directory containing the model is available\n","#!ls"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71dd9a6a021d4ffcc159dafd52e7f86ecc6558cb","id":"ewhXmnWl5zmr"},"cell_type":"code","source":["# view the files that make up the tensorflow.js model\n","#os.listdir('tfjs_dir')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f774cd15c6de188d4bb150f25ab600e5cbc06031","id":"xfWX6zL75zms"},"cell_type":"code","source":["# Delete the image data directory we created to prevent a Kaggle error.\n","# Kaggle allows a max of 500 files to be saved.\n","\n","shutil.rmtree('base_dir')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b6056bb27006ebc85fb54bbc8b9b989bd756ff1","id":"x_TkvmuA5zms"},"cell_type":"markdown","source":["### Resources\n","\n","These are some resources that I used:"]},{"metadata":{"_uuid":"d5f5d88e7cda18fb86c7e9715e488536e4424673","id":"3wzqKkuY5zms"},"cell_type":"markdown","source":["1. Excellent tutorial series by deeplizard on how to use Mobilenet with Tensorflow.js<br>\n","https://www.youtube.com/watch?v=HEQDRWMK6yY\n","\n","2. Tutorial by Minsuk Heo on Accuracy, Precision and F1 Score<br>\n","https://www.youtube.com/watch?v=HBi-P5j0Kec\n","\n","3. Tutorial by Data School on how to evaluate a classifier<br>\n","https://www.youtube.com/watch?v=85dtiMz9tSo\n","\n","3. Tensorflow.js gallery of projects<br>\n","https://github.com/tensorflow/tfjs/blob/master/GALLERY.md\n","\n"]},{"metadata":{"_uuid":"55623033f552beec0101f2d8241c122404b1f82f","id":"u0fbJ69E5zms"},"cell_type":"markdown","source":["### Conclusion"]},{"metadata":{"_uuid":"8d99df295cd21dfd7f15176a35eabd1ed0d41cdb","id":"phdWtP_a5zms"},"cell_type":"markdown","source":["Many thanks to Kevin Mader (@kmader) for posting this dataset. Thanks Kaggle for the free GPU.\n","\n","Thank you for reading. "]},{"metadata":{"trusted":true,"_uuid":"ffb96375ce44ef009192bf6ddb44f6eb53a43838","id":"0TAmj1n25zms"},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}